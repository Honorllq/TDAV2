#!/usr/bin/env python
"""
å¤šæ•°æ®é›†ç½‘æ ¼æœç´¢è„šæœ¬ - è‡ªåŠ¨æ‰¾åˆ°TDA+GR-CLIPçš„æœ€ä¼˜è¶…å‚æ•°

ä½¿ç”¨æ–¹æ³•:
    # å•æ•°æ®é›†
    python grid_search_multi.py --datasets caltech101 --mode quick

    # å¤šæ•°æ®é›† (ç”¨/åˆ†éš”)
    python grid_search_multi.py --datasets caltech101/dtd/eurosat --mode quick

    # OODåŸºå‡†
    python grid_search_multi.py --datasets I/A/V/R/S --backbone RN50 --mode grclip_only

    # è·¨åŸŸåŸºå‡† (10ä¸ªæ•°æ®é›†)
    python grid_search_multi.py --datasets caltech101/dtd/eurosat/fgvc/food101/oxford_flowers/oxford_pets/stanford_cars/sun397/ucf101 --mode tda_only

è¾“å‡º:
    results/grid_search_YYYYMMDD_HHMMSS/
    â”œâ”€â”€ summary_all_datasets.csv       # æ‰€æœ‰æ•°æ®é›†æ±‡æ€»
    â”œâ”€â”€ best_configs_summary.txt       # æ¯ä¸ªæ•°æ®é›†çš„æœ€ä½³é…ç½®
    â”œâ”€â”€ caltech101/
    â”‚   â”œâ”€â”€ results.csv
    â”‚   â””â”€â”€ best_config.txt
    â”œâ”€â”€ dtd/
    â”‚   â”œâ”€â”€ results.csv
    â”‚   â””â”€â”€ best_config.txt
    â””â”€â”€ ...
"""

import os
import sys
import argparse
import itertools
from datetime import datetime
from pathlib import Path
import pandas as pd
import random
import torch
import clip
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from utils import *
from runner import update_cache, compute_cache_logits, build_visual_prototypes, compute_modality_means, calibrate_with_gr_clip


def run_single_experiment(config, dataset_name, data_root, clip_model, clip_weights, preprocess):
    """è¿è¡Œå•æ¬¡å®éªŒ"""
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    test_loader, _, _ = build_test_data_loader(dataset_name, data_root, preprocess)

    with torch.no_grad():
        pos_cache, neg_cache, accuracies = {}, {}, []

        # æå–é…ç½®
        pos_cfg = config['positive']
        neg_cfg = config['negative']
        cal_cfg = config['calibrate']

        text_mean, image_mean = None, None
        calibrated_weights = clip_weights

        for i, (images, target) in enumerate(tqdm(test_loader, desc='æµ‹è¯•è¿›åº¦', leave=False)):
            # æ­¥éª¤1: ç‰¹å¾æå–
            image_features, clip_logits, loss, prob_map, pred = get_clip_logits(images, clip_model, clip_weights)
            target = target.cuda()
            prop_entropy = get_entropy(loss, clip_weights)

            # æ­¥éª¤2: æ›´æ–°æ­£ç¼“å­˜
            if pos_cfg['enabled']:
                update_cache(pos_cache, pred, [image_features, loss], pos_cfg['shot_capacity'])

            # æ­¥éª¤3: æ›´æ–°è´Ÿç¼“å­˜
            if neg_cfg['enabled']:
                if neg_cfg['entropy_threshold']['lower'] < prop_entropy < neg_cfg['entropy_threshold']['upper']:
                    update_cache(neg_cache, pred, [image_features, loss, prob_map],
                               neg_cfg['shot_capacity'], True)

            # æ­¥éª¤4: GR-CLIPæ ¡å‡†
            if cal_cfg['enabled'] and len(pos_cache) >= cal_cfg['min_cache_size']:
                if i % cal_cfg['update_interval'] == 0:
                    text_mean, image_mean = compute_modality_means(pos_cache, clip_weights)
                    visual_prototypes = build_visual_prototypes(pos_cache)
                    calibrated_weights = calibrate_with_gr_clip(
                        visual_prototypes, clip_weights, text_mean, image_mean,
                        alpha=cal_cfg['fusion_alpha']
                    )

            # æ­¥éª¤5: è®¡ç®—é¢„æµ‹
            clip_baseline_logits = 100.0 * image_features @ clip_weights

            if text_mean is not None and image_mean is not None and cal_cfg['enabled']:
                # è‡ªé€‚åº”GR-CLIP
                clip_confidence = clip_baseline_logits.softmax(1).max().item()
                if clip_confidence < cal_cfg.get('confidence_threshold', 1.0):
                    image_features_centered = image_features - image_mean.unsqueeze(0)
                    image_features_centered = image_features_centered / image_features_centered.norm(dim=1, keepdim=True)
                    gr_clip_logits = 100.0 * image_features_centered @ calibrated_weights
                else:
                    gr_clip_logits = clip_baseline_logits
            else:
                gr_clip_logits = clip_baseline_logits

            # æ­¥éª¤6: TDAç¼“å­˜
            tda_logits = torch.zeros_like(gr_clip_logits)
            if pos_cfg['enabled'] and pos_cache:
                tda_logits += compute_cache_logits(image_features, pos_cache,
                                                   pos_cfg['alpha'], pos_cfg['beta'], clip_weights)
            if neg_cfg['enabled'] and neg_cache:
                tda_logits -= compute_cache_logits(image_features, neg_cache,
                                                   neg_cfg['alpha'], neg_cfg['beta'], clip_weights,
                                                   (neg_cfg['mask_threshold']['lower'],
                                                    neg_cfg['mask_threshold']['upper']))

            # æ­¥éª¤7: èåˆé¢„æµ‹
            final_logits = gr_clip_logits + tda_logits

            # è¯„ä¼°
            acc = cls_acc(final_logits, target)
            accuracies.append(acc)

        avg_acc = sum(accuracies) / len(accuracies)
        return avg_acc


def grid_search_single_dataset(dataset_name, args, search_space, clip_model, preprocess, dataset_dir):
    """å¯¹å•ä¸ªæ•°æ®é›†æ‰§è¡Œç½‘æ ¼æœç´¢"""

    print(f"\n{'='*80}")
    print(f"ğŸ¯ æ•°æ®é›†: {dataset_name}")
    print(f"{'='*80}")

    # åŠ è½½æ•°æ®é›†å’Œæ–‡æœ¬ç‰¹å¾
    print(f"ğŸ“¥ åŠ è½½æ•°æ®é›†é…ç½®...")
    cfg = get_config_file(args.config_dir, dataset_name)
    test_loader, classnames, template = build_test_data_loader(dataset_name, args.data_root, preprocess)
    clip_weights = clip_classifier(classnames, template, clip_model)

    # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
    param_names = list(search_space.keys())
    param_values = list(search_space.values())
    all_combinations = list(itertools.product(*param_values))

    total_exps = len(all_combinations)
    print(f"ï¿½ï¿½ è¯¥æ•°æ®é›†å®éªŒæ•°: {total_exps}")

    # åŸºç¡€é…ç½®
    base_config = {
        'positive': {
            'enabled': True,
            'shot_capacity': 3,
            'alpha': 2.0,
            'beta': 5.0
        },
        'negative': {
            'enabled': True,
            'shot_capacity': 2,
            'alpha': 0.117,
            'beta': 1.0,
            'entropy_threshold': {'lower': 0.2, 'upper': 0.5},
            'mask_threshold': {'lower': 0.03, 'upper': 1.0}
        },
        'calibrate': {
            'enabled': True,
            'min_cache_size': 20,
            'update_interval': 100,
            'fusion_alpha': 0.8,
            'confidence_threshold': 0.7
        }
    }

    # å­˜å‚¨ç»“æœ
    results = []
    best_acc = 0
    best_params = None
    best_config = None

    # éå†æ‰€æœ‰ç»„åˆ
    for exp_id, combination in enumerate(all_combinations, 1):
        params = dict(zip(param_names, combination))

        print(f"\n{'â”€'*80}")
        print(f"ğŸ§ª [{dataset_name}] å®éªŒ {exp_id}/{total_exps}")
        print(f"{'â”€'*80}")

        # åˆ›å»ºå½“å‰é…ç½®
        config = {
            'positive': base_config['positive'].copy(),
            'negative': base_config['negative'].copy(),
            'calibrate': base_config['calibrate'].copy()
        }

        # æ›´æ–°å‚æ•°
        for key, value in params.items():
            if key.startswith('pos_'):
                config['positive'][key.replace('pos_', '')] = value
            elif key.startswith('neg_'):
                config['negative'][key.replace('neg_', '')] = value
            elif key.startswith('cal_'):
                config['calibrate'][key.replace('cal_', '')] = value

        # æ‰“å°å½“å‰é…ç½®
        print(f"å‚æ•°: {params}")

        # è¿è¡Œå®éªŒ
        try:
            accuracy = run_single_experiment(
                config, dataset_name, args.data_root, clip_model, clip_weights, preprocess
            )
            print(f"âœ… å‡†ç¡®ç‡: {accuracy:.2f}%")

            # è®°å½•ç»“æœ
            result = params.copy()
            result['accuracy'] = accuracy
            result['dataset'] = dataset_name
            result['exp_id'] = exp_id
            results.append(result)

            # æ›´æ–°æœ€ä½³ç»“æœ
            if accuracy > best_acc:
                best_acc = accuracy
                best_params = params.copy()
                best_config = config
                print(f"ğŸŒŸ [{dataset_name}] æ–°çš„æœ€ä½³ç»“æœ! {best_acc:.2f}%")

        except Exception as e:
            print(f"âŒ å®éªŒå¤±è´¥: {e}")
            result = params.copy()
            result['accuracy'] = None
            result['dataset'] = dataset_name
            result['exp_id'] = exp_id
            result['error'] = str(e)
            results.append(result)

        # ä¿å­˜ä¸­é—´ç»“æœ
        df = pd.DataFrame(results)
        df.to_csv(dataset_dir / "results_partial.csv", index=False)

    # ä¿å­˜æ•°æ®é›†æœ€ç»ˆç»“æœ
    df = pd.DataFrame(results)
    df = df.sort_values('accuracy', ascending=False)
    df.to_csv(dataset_dir / "results.csv", index=False)

    # ä¿å­˜æœ€ä½³é…ç½®
    if best_config:
        with open(dataset_dir / "best_config.txt", 'w', encoding='utf-8') as f:
            f.write(f"Dataset: {dataset_name}\n")
            f.write(f"Best Accuracy: {best_acc:.2f}%\n\n")
            f.write("Best Parameters:\n")
            for k, v in best_params.items():
                f.write(f"  {k}: {v}\n")
            f.write(f"\nFull Configuration:\n")
            f.write(f"positive:\n")
            for k, v in best_config['positive'].items():
                f.write(f"  {k}: {v}\n")
            f.write(f"\nnegative:\n")
            for k, v in best_config['negative'].items():
                f.write(f"  {k}: {v}\n")
            f.write(f"\ncalibrate:\n")
            for k, v in best_config['calibrate'].items():
                f.write(f"  {k}: {v}\n")

    print(f"\nâœ¨ [{dataset_name}] å®Œæˆ! æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")

    return df, best_params, best_acc


def main():
    parser = argparse.ArgumentParser(description='å¤šæ•°æ®é›†TDA+GR-CLIPç½‘æ ¼æœç´¢')
    parser.add_argument('--datasets', type=str, default='caltech101',
                        help='æ•°æ®é›†åç§°,ç”¨/åˆ†éš”å¤šä¸ª (å¦‚: caltech101/dtd/eurosat)')
    parser.add_argument('--backbone', type=str, default='ViT-B/16',
                        choices=['RN50', 'ViT-B/16'],
                        help='CLIP backbone')
    parser.add_argument('--config-dir', type=str, default='configs',
                        help='é…ç½®ç›®å½•')
    parser.add_argument('--data-root', type=str,
                        default='E:/ç ”ç©¶ç”Ÿ/æå‰è¿›ç»„/æç¤ºå­¦ä¹ æ–¹å‘/code/dataset/',
                        help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--mode', type=str, default='quick',
                        choices=['quick', 'full', 'grclip_only', 'tda_only'],
                        help='æœç´¢æ¨¡å¼')
    args = parser.parse_args()

    # è§£ææ•°æ®é›†åˆ—è¡¨
    dataset_list = args.datasets.split('/')

    # åˆ›å»ºç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_dir = Path("results") / f"grid_search_{timestamp}"
    result_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*80}")
    print(f"ğŸ” å¤šæ•°æ®é›†ç½‘æ ¼æœç´¢")
    print(f"{'='*80}")
    print(f"ğŸ“ ç»“æœç›®å½•: {result_dir}")
    print(f"ğŸ¯ æ•°æ®é›†: {', '.join(dataset_list)} ({len(dataset_list)}ä¸ª)")
    print(f"ğŸ§  Backbone: {args.backbone}")
    print(f"ğŸ”§ æœç´¢æ¨¡å¼: {args.mode}")
    print(f"{'='*80}")

    # åˆå§‹åŒ–CLIPæ¨¡å‹
    print(f"\nğŸ“¥ åŠ è½½CLIPæ¨¡å‹...")
    clip_model, preprocess = clip.load(args.backbone)
    clip_model.eval()

    # è®¾ç½®éšæœºç§å­
    random.seed(1)
    torch.manual_seed(1)

    # å®šä¹‰æœç´¢ç©ºé—´
    if args.mode == 'quick':
        search_space = {
            'pos_alpha': [1.5, 2.0, 2.5],
            'cal_enabled': [True, False],
            'cal_fusion_alpha': [0.8, 0.9],
        }
    elif args.mode == 'grclip_only':
        search_space = {
            'cal_enabled': [False],
            'cal_fusion_alpha': [0.3, 0.5, 0.7, 0.8, 0.9],
            'cal_confidence_threshold': [0.6, 0.7, 0.8, 0.9],
            'cal_min_cache_size': [10, 20, 30],
        }
    elif args.mode == 'tda_only':
        search_space = {
            'pos_shot_capacity': [2, 3, 4],
            'pos_alpha': [1.0, 1.5, 2.0, 2.5, 3.0],
            'pos_beta': [4.0, 5.0, 6.0],
            'neg_alpha': [0.1, 0.117, 0.15],
            'cal_enabled': [False],
        }
    elif args.mode == 'full':
        search_space = {
            'pos_shot_capacity': [2, 3, 4],
            'pos_alpha': [1.5, 2.0, 2.5],
            'pos_beta': [4.0, 5.0, 6.0],
            'neg_alpha': [0.1, 0.117],
            'cal_enabled': [True, False],
            'cal_fusion_alpha': [0.7, 0.8, 0.9],
            'cal_confidence_threshold': [0.6, 0.7, 0.8],
        }

    # å­˜å‚¨æ‰€æœ‰æ•°æ®é›†çš„ç»“æœ
    all_results = []
    summary = []

    # å¯¹æ¯ä¸ªæ•°æ®é›†æ‰§è¡Œç½‘æ ¼æœç´¢
    for dataset_name in dataset_list:
        # åˆ›å»ºæ•°æ®é›†ç›®å½•
        dataset_dir = result_dir / dataset_name
        dataset_dir.mkdir(exist_ok=True)

        # æ‰§è¡Œæœç´¢
        df, best_params, best_acc = grid_search_single_dataset(
            dataset_name, args, search_space, clip_model, preprocess, dataset_dir
        )

        # æ”¶é›†ç»“æœ
        all_results.append(df)
        summary.append({
            'dataset': dataset_name,
            'best_accuracy': best_acc,
            'best_params': str(best_params)
        })

    # ä¿å­˜æ±‡æ€»ç»“æœ
    print(f"\n{'='*80}")
    print(f"âœ¨ æ‰€æœ‰æ•°æ®é›†æœç´¢å®Œæˆ!")
    print(f"{'='*80}")

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    combined_df = pd.concat(all_results, ignore_index=True)
    combined_df.to_csv(result_dir / "summary_all_datasets.csv", index=False)

    # ä¿å­˜æ¯ä¸ªæ•°æ®é›†çš„æœ€ä½³é…ç½®
    summary_df = pd.DataFrame(summary)
    summary_df.to_csv(result_dir / "best_accuracies_per_dataset.csv", index=False)

    with open(result_dir / "best_configs_summary.txt", 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("æ¯ä¸ªæ•°æ®é›†çš„æœ€ä½³é…ç½®\n")
        f.write("=" * 80 + "\n\n")
        for item in summary:
            f.write(f"æ•°æ®é›†: {item['dataset']}\n")
            f.write(f"æœ€ä½³å‡†ç¡®ç‡: {item['best_accuracy']:.2f}%\n")
            f.write(f"æœ€ä½³å‚æ•°: {item['best_params']}\n")
            f.write("-" * 80 + "\n\n")

    print(f"\nğŸ“Š æ±‡æ€»ç»“æœ:")
    print(summary_df.to_string(index=False))
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_dir}")
    print(f"  - summary_all_datasets.csv: æ‰€æœ‰å®éªŒç»“æœ")
    print(f"  - best_configs_summary.txt: æ¯ä¸ªæ•°æ®é›†çš„æœ€ä½³é…ç½®")
    print(f"  - æ¯ä¸ªæ•°æ®é›†ç›®å½•ä¸‹æœ‰è¯¦ç»†ç»“æœ")


if __name__ == "__main__":
    main()
