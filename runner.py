"""
TDA (è®­ç»ƒè‡ªç”±åŠ¨æ€é€‚é…å™¨) å®ç°

æœ¬æ¨¡å—å®ç°äº†è®ºæ–‡"Efficient Test-Time Adaptation of Vision-Language Models"ä¸­æè¿°çš„æ ¸å¿ƒç®—æ³•

è®ºæ–‡æ ¸å¿ƒæ¦‚å¿µ:
- åŠ¨æ€é€‚é…å™¨: ä½¿ç”¨é”®å€¼ç¼“å­˜çš„æ— è®­ç»ƒæœºåˆ¶
- æ­£ç¼“å­˜: å­˜å‚¨é«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾åŠå¯¹åº”ç‰¹å¾
- è´Ÿç¼“å­˜: å­˜å‚¨è´Ÿä¼ªæ ‡ç­¾ä»¥å‡å°‘å™ªå£°å½±å“  
- æµ‹è¯•æ—¶é€‚åº”: æ— éœ€åå‘ä¼ æ’­å³å¯é€‚åº”åˆ†å¸ƒåç§»

è¯¥æ–¹æ³•é€šè¿‡é¿å…æ¢¯åº¦è®¡ç®—ï¼Œåœ¨ä¿æŒæˆ–æå‡ç²¾åº¦çš„åŒæ—¶å®ç°äº†ç›¸æ¯”TPT/DiffTPTçš„ä¼˜å¼‚æ•ˆç‡
"""

import random
import argparse
import wandb
from tqdm import tqdm
from datetime import datetime

import torch
import torch.nn.functional as F
import operator

import clip
from utils import *

# VS Codeè°ƒè¯•é…ç½®
# import debugpy
# try:
#     # å¯ç”¨è¿œç¨‹è°ƒè¯•ç”¨äºå¼€å‘
#     debugpy.listen(("localhost", 9508))
#     print("ç­‰å¾…è°ƒè¯•å™¨è¿æ¥")
#     debugpy.wait_for_client()
# except Exception as e:
#     pass


def get_arguments():
    """è·å–æµ‹è¯•æ—¶é€‚åº”çš„å‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', dest='config', required=True, help='settings of TDA on specific dataset in yaml format.')
    parser.add_argument('--wandb-log', dest='wandb', action='store_true', help='Whether you want to log to wandb. Include this flag to enable logging.')
    parser.add_argument('--datasets', dest='datasets', type=str, required=True, help="Datasets to process, separated by a slash (/). Example: I/A/V/R/S")
    parser.add_argument('--data-root', dest='data_root', type=str, default='E:/ç ”ç©¶ç”Ÿ/æå‰è¿›ç»„/æç¤ºå­¦ä¹ æ–¹å‘/code/dataset/', help='Path to the datasets directory. Default is ./dataset/')
    parser.add_argument('--backbone', dest='backbone', type=str, choices=['RN50', 'ViT-B/16'], required=True, help='CLIP model backbone to use: RN50 or ViT-B/16.')

    args = parser.parse_args()

    return args


def update_cache(cache, pred, features_loss, shot_capacity, include_prob_map=False):
    """
    æ›´æ–°åŠ¨æ€ç¼“å­˜ï¼Œæ·»åŠ æ–°çš„ç‰¹å¾å’Œç†µæŸå¤±
    
    å®ç°è®ºæ–‡3.2èŠ‚æè¿°çš„åŠ¨æ€é˜Ÿåˆ—æœºåˆ¶ã€‚ç¼“å­˜ä¸ºæ¯ä¸ªç±»åˆ«ç»´æŠ¤k-shotæ ·æœ¬ï¼ŒæŒ‰é¢„æµ‹ç†µæ’åºã€‚
    
    è®ºæ–‡å¼•ç”¨: "ç»™å®šæµ‹è¯•æ ·æœ¬ï¼ŒTDAå°†åŸºäºç†µæ¡ä»¶æ·»åŠ /æ›¿æ¢é”®å€¼å¯¹ä»¥ç¡®ä¿é«˜è´¨é‡ä¼ªæ ‡ç­¾"
    
    å‚æ•°:
        cache (dict): åŠ¨æ€ç¼“å­˜ {ç±»åˆ«ID: [(ç‰¹å¾, ç†µ, æ¦‚ç‡å›¾), ...]}
        pred (int): é¢„æµ‹ç±»åˆ«æ ‡ç­¾(ä¼ªæ ‡ç­¾)  
        features_loss (list): [å›¾åƒç‰¹å¾, ç†µæŸå¤±, æ¦‚ç‡å›¾(å¯é€‰)]
            - image_features: CLIPç¼–ç ç‰¹å¾ [1, d] å…¶ä¸­dä¸ºç‰¹å¾ç»´åº¦(512)
            - entropy_loss: é¢„æµ‹ç†µ (æ ‡é‡) 
            - prob_map: ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ [1, num_classes] (ç”¨äºè´Ÿç¼“å­˜)
        shot_capacity (int): æ¯ç±»æœ€å¤§æ ·æœ¬æ•° (è®ºæ–‡ä¸­çš„k)
        include_prob_map (bool): æ˜¯å¦åŒ…å«æ¦‚ç‡å›¾ (è´Ÿç¼“å­˜ç”¨)
    
    ç¼“å­˜ç»“æ„:
        æ­£ç¼“å­˜: [(ç‰¹å¾, ç†µ), ...]  
        è´Ÿç¼“å­˜: [(ç‰¹å¾, ç†µ, æ¦‚ç‡å›¾), ...]
    """
    with torch.no_grad():
        # æ ¹æ®ç¼“å­˜ç±»å‹(æ­£/è´Ÿ)å‡†å¤‡ç¼“å­˜é¡¹
        item = features_loss if not include_prob_map else features_loss[:2] + [features_loss[2]]
        
        if pred in cache:
            # ç±»åˆ«å·²å­˜åœ¨äºç¼“å­˜ä¸­
            if len(cache[pred]) < shot_capacity:
                # æ¡ä»¶1: æ ·æœ¬æ•° < æœ€å¤§å®¹é‡k
                # ç›´æ¥æ·»åŠ æ–°çš„é”®å€¼å¯¹åˆ°ç¼“å­˜
                cache[pred].append(item)
            elif features_loss[1] < cache[pred][-1][1]:
                # æ¡ä»¶2: æ ·æœ¬æ•° = æœ€å¤§å®¹é‡k
                # å¦‚æœå½“å‰ç†µæ›´ä½åˆ™æ›¿æ¢æœ€é«˜ç†µé¡¹
                # ç¡®ä¿ä¿ç•™æœ€æœ‰ä¿¡å¿ƒçš„é¢„æµ‹
                cache[pred][-1] = item
            
            # ç»´æŠ¤åŸºäºç†µçš„æ’åº(ä¼˜å…ˆé˜Ÿåˆ—è¡Œä¸º)
            # ä½ç†µ = é«˜ç½®ä¿¡åº¦ = é«˜ä¼˜å…ˆçº§
            cache[pred] = sorted(cache[pred], key=operator.itemgetter(1))
        else:
            # è¯¥ç±»åˆ«çš„ç¬¬ä¸€ä¸ªæ ·æœ¬
            cache[pred] = [item]


def compute_cache_logits(image_features, cache, alpha, beta, clip_weights, neg_mask_thresholds=None):
    """
    ä½¿ç”¨å­˜å‚¨çš„é”®å€¼å¯¹è®¡ç®—åŸºäºç¼“å­˜çš„logits
    
    å®ç°è®ºæ–‡å…¬å¼3ã€6:
    - æ­£ç¼“å­˜: P_pos(f_test) = A(f_test @ Q_p^T) @ L_p  (å…¬å¼3)
    - è´Ÿç¼“å­˜: P_neg(f_test) = -A(f_test @ Q_n^T) @ L_n  (å…¬å¼6)
    
    å…¶ä¸­ A(z) = Î± * exp(-Î²(1-z)) æ˜¯æ¥è‡ªTip-Adapterçš„é€‚é…å‡½æ•°
    
    å‚æ•°:
        image_features: æµ‹è¯•å›¾åƒç‰¹å¾ [1, d] æ¥è‡ªCLIPç¼–ç å™¨ï¼Œd=512
        cache: åŒ…å«å­˜å‚¨ç‰¹å¾å’Œæ ‡ç­¾çš„åŠ¨æ€ç¼“å­˜
        alpha: æ®‹å·®æ¯”ç‡(Î±) - ç¼“å­˜é¢„æµ‹çš„æƒé‡å› å­  
        beta: é”åº¦æ¯”ç‡(Î²) - æ§åˆ¶ç›¸ä¼¼åº¦é”åŒ–ç¨‹åº¦
        clip_weights: CLIPæ–‡æœ¬åµŒå…¥ [d, num_classes]
        neg_mask_thresholds: è´Ÿä¼ªæ ‡ç­¾çš„(ä¸‹ç•Œ,ä¸Šç•Œ)é˜ˆå€¼
    
    è¿”å›:
        cache_logits: [1, num_classes] æ¥è‡ªç¼“å­˜é€‚é…çš„logits
    
    æ•°æ®æµç¨‹:
        1. æå–ç¼“å­˜é”®Qå’Œå€¼L 
        2. è®¡ç®—ç›¸ä¼¼åº¦: f_test @ Q^T [1, num_cached_samples]
        3. åº”ç”¨é€‚é…å‡½æ•°A(ç›¸ä¼¼åº¦)
        4. è®¡ç®—æœ€ç»ˆlogits: A(ç›¸ä¼¼åº¦) @ L
    """
    with torch.no_grad():
        cache_keys = []    # å°†å­˜å‚¨Q_pæˆ–Q_n: ç¼“å­˜çš„å›¾åƒç‰¹å¾
        cache_values = []  # å°†å­˜å‚¨L_pæˆ–L_n: ä¼ªæ ‡ç­¾
        
        # ä¸ºæ‰€æœ‰ç±»åˆ«æå–ç¼“å­˜ç‰¹å¾(é”®)å’Œæ ‡ç­¾(å€¼)
        for class_index in sorted(cache.keys()):
            for item in cache[class_index]:
                cache_keys.append(item[0])  # image_features [1, d]
                if neg_mask_thresholds:
                    # è´Ÿç¼“å­˜: ä½¿ç”¨æ¦‚ç‡å›¾è¿›è¡Œè´Ÿä¼ªæ ‡ç­¾
                    cache_values.append(item[2])  # prob_map [1, num_classes]
                else:
                    # æ­£ç¼“å­˜: ä½¿ç”¨ç±»åˆ«ç´¢å¼•ä½œä¸ºone-hotæ ‡ç­¾
                    cache_values.append(class_index)  # class_id (æ ‡é‡)

        # å‡†å¤‡ç¼“å­˜é”®: Q^T å½¢çŠ¶ä¸º [d, num_cached_samples]
        cache_keys = torch.cat(cache_keys, dim=0).permute(1, 0)
        
        if neg_mask_thresholds:
            # è´Ÿç¼“å­˜å¤„ç† (è®ºæ–‡å…¬å¼4)
            # L_n = -1[p_l < P(Q_n)] å…¶ä¸­p_læ˜¯é˜ˆå€¼
            cache_values = torch.cat(cache_values, dim=0)  # [num_cached_samples, num_classes]
            
            # åº”ç”¨è´Ÿæ©ç : å€¼>é˜ˆå€¼å˜ä¸º1ï¼Œå…¶ä»–å˜ä¸º0
            # è¿™å®ç°äº†ä¸ç¡®å®šé¢„æµ‹çš„è´Ÿä¼ªæ ‡ç­¾
            cache_values = (((cache_values > neg_mask_thresholds[0]) & 
                           (cache_values < neg_mask_thresholds[1])).type(torch.int8)).cuda().half()
        else:
            # æ­£ç¼“å­˜å¤„ç†
            # å°†ç±»åˆ«ç´¢å¼•è½¬æ¢ä¸ºone-hotç¼–ç æ ‡ç­¾L_p
            cache_values = (F.one_hot(torch.Tensor(cache_values).to(torch.int64), 
                                    num_classes=clip_weights.size(1))).cuda().half()

        # è®¡ç®—æµ‹è¯•ç‰¹å¾å’Œç¼“å­˜ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦
        # affinity = f_test @ Q^T å½¢çŠ¶ä¸º [1, num_cached_samples]
        affinity = image_features @ cache_keys
        
        # åº”ç”¨é€‚é…å‡½æ•° A(z) = Î± * exp(-Î²(1-z))
        # è¿™æ˜¯æ ¸å¿ƒçš„åŸºäºç›¸ä¼¼åº¦çš„æƒé‡æœºåˆ¶
        adaptation_weights = ((-1) * (beta - beta * affinity)).exp()
        
        # è®¡ç®—æœ€ç»ˆç¼“å­˜logits: A(ç›¸ä¼¼åº¦) @ L
        # å½¢çŠ¶: [1, num_cached_samples] @ [num_cached_samples, num_classes] = [1, num_classes]
        cache_logits = adaptation_weights @ cache_values
        
        # åº”ç”¨æ®‹å·®æ¯”ç‡Î±æ¥ç¼©æ”¾ç¼“å­˜è´¡çŒ®
        return alpha * cache_logits
def build_visual_prototypes(cache):
    """æ„å»ºè§†è§‰åŸå‹"""
    prototypes = {}
    for class_id in cache.keys():
        features = [item[0].squeeze(0) for item in cache[class_id]]
        prototypes[class_id] = torch.stack(features, dim=0).mean(dim=0)
    return prototypes

def compute_modality_means(pos_cache, clip_weights):
    """
    è®¡ç®—æ¨¡æ€å‡å€¼ (GR-CLIPæ ¸å¿ƒ)
    
    è®ºæ–‡æ–¹æ³•: ä»æ ¡å‡†é›†è®¡ç®—query/text/imageçš„å…¨å±€å‡å€¼
    æˆ‘ä»¬çš„å®ç°: ä»æµ‹è¯•ç¼“å­˜åŠ¨æ€è®¡ç®— (æ›´é€‚åˆTDAåœºæ™¯)
    
    è¿”å›:
        text_mean: æ–‡æœ¬ç‰¹å¾å‡å€¼ [512]
        image_mean: å›¾åƒç‰¹å¾å‡å€¼ [512]
    """
    with torch.no_grad():
        # ä»ç¼“å­˜æå–æ‰€æœ‰ç‰¹å¾
        all_image_features = []
        for class_id in pos_cache.keys():
            for item in pos_cache[class_id]:
                all_image_features.append(item[0].squeeze(0))  # [512]
        
        if len(all_image_features) == 0:
            # ç¼“å­˜ä¸ºç©º,è¿”å›é›¶å‘é‡
            # âœ… ä¿®å¤1: åŠ¨æ€è·å–ç‰¹å¾ç»´åº¦ (RN50=1024, ViT-B/16=512)
            feature_dim = clip_weights.size(0)
            return torch.zeros(feature_dim).cuda(), torch.zeros(feature_dim).cuda()
        
        # è®¡ç®—å›¾åƒç‰¹å¾å‡å€¼
        image_mean = torch.stack(all_image_features).mean(dim=0)  # [512]
        
        # è®¡ç®—æ–‡æœ¬ç‰¹å¾å‡å€¼ (ä»CLIPæƒé‡)
        text_mean = clip_weights.mean(dim=1)  # [512]
        
        return text_mean, image_mean

def calibrate_with_gr_clip(visual_prototypes, clip_weights, text_mean, image_mean, alpha=0.7):
    """
    ä½¿ç”¨GR-CLIPæ–¹æ³•æ ¡å‡†: å»é™¤æ¨¡æ€é—´éš™åèåˆ

    æ­¥éª¤:
    1. å»ä¸­å¿ƒåŒ–: å‡å»æ¨¡æ€å‡å€¼
    2. æ ‡å‡†åŒ–: L2å½’ä¸€åŒ–
    3. èåˆ: åœ¨å»ååçš„ç©ºé—´ä¸­èåˆ

    å‚æ•°:
        visual_prototypes: è§†è§‰åŸå‹ {class_id: [d]}
        clip_weights: æ–‡æœ¬ç‰¹å¾ [d, num_classes]
        text_mean: æ–‡æœ¬å‡å€¼ [d]
        image_mean: å›¾åƒå‡å€¼ [d]
        alpha: èåˆæƒé‡ (æ–‡æœ¬å æ¯”, é»˜è®¤0.7)

    è¿”å›:
        calibrated_weights: æ ¡å‡†åçš„æƒé‡ [d, num_classes]
    """
    with torch.no_grad():
        num_classes = clip_weights.size(1)
        calibrated_weights = torch.zeros_like(clip_weights)  # [512, num_classes]
        
        for class_id in range(num_classes):
            # æå–åŸå§‹ç‰¹å¾
            T_c = clip_weights[:, class_id]  # æ–‡æœ¬ç‰¹å¾ [512]
            
            if class_id in visual_prototypes:
                V_c = visual_prototypes[class_id]  # è§†è§‰åŸå‹ [512]
                
                # ğŸ”‘ GR-CLIPæ ¸å¿ƒ: å»ä¸­å¿ƒåŒ–
                T_c_centered = T_c - text_mean
                V_c_centered = V_c - image_mean
                
                # æ ‡å‡†åŒ–
                T_c_centered = T_c_centered / T_c_centered.norm()
                V_c_centered = V_c_centered / V_c_centered.norm()

                # èåˆ (ç°åœ¨åœ¨åŒä¸€ç©ºé—´ä¸­)
                # âœ… ä¿®å¤2: ä½¿ç”¨ä¼ å…¥çš„alphaå‚æ•°è€Œéç¡¬ç¼–ç 
                fused = alpha * T_c_centered + (1 - alpha) * V_c_centered
                
                # æœ€ç»ˆæ ‡å‡†åŒ–
                calibrated_weights[:, class_id] = fused / fused.norm()
            else:
                # æ— è§†è§‰åŸå‹,ä»…å»ä¸­å¿ƒåŒ–æ–‡æœ¬
                T_c_centered = T_c - text_mean
                calibrated_weights[:, class_id] = T_c_centered / T_c_centered.norm()
        
        return calibrated_weights


def run_test_tda(pos_cfg, neg_cfg, calibrate_cfg, loader, clip_model, clip_weights):
    """
    TDA + GR-CLIPé›†æˆç‰ˆæœ¬
    """
    with torch.no_grad():
        pos_cache, neg_cache, accuracies = {}, {}, []
        
        pos_enabled, neg_enabled = pos_cfg['enabled'], neg_cfg['enabled']
        if pos_enabled:
            pos_params = {k: pos_cfg[k] for k in ['shot_capacity', 'alpha', 'beta']}
        if neg_enabled:
            neg_params = {k: neg_cfg[k] for k in ['shot_capacity', 'alpha', 'beta', 'entropy_threshold', 'mask_threshold']}

        # åˆå§‹åŒ–æ¨¡æ€å‡å€¼ (GR-CLIP)
        text_mean, image_mean = None, None
        calibrated_weights = clip_weights

        for i, (images, target) in enumerate(tqdm(loader, desc='å·²å¤„ç†æµ‹è¯•å›¾åƒ: ')):
            # æ­¥éª¤1: ç‰¹å¾æå–
            image_features, clip_logits, loss, prob_map, pred = get_clip_logits(images, clip_model, clip_weights)
            target, prop_entropy = target.cuda(), get_entropy(loss, clip_weights)

            # æ­¥éª¤2: æ›´æ–°ç¼“å­˜
            if pos_enabled:
                update_cache(pos_cache, pred, [image_features, loss], pos_params['shot_capacity'])
            if neg_enabled and neg_params['entropy_threshold']['lower'] < prop_entropy < neg_params['entropy_threshold']['upper']:
                update_cache(neg_cache, pred, [image_features, loss, prob_map], neg_params['shot_capacity'], True)

            # æ­¥éª¤3: GR-CLIPæ ¡å‡† (å®šæœŸæ›´æ–°)
            if calibrate_cfg['enabled'] and len(pos_cache) >= calibrate_cfg['min_cache_size']:
                if i % calibrate_cfg.get('update_interval', 100) == 0:
                    # è®¡ç®—æ¨¡æ€å‡å€¼
                    text_mean, image_mean = compute_modality_means(pos_cache, clip_weights)

                    # æ„å»ºè§†è§‰åŸå‹
                    visual_prototypes = build_visual_prototypes(pos_cache)

                    # âœ… ä¿®å¤2: ä»é…ç½®è¯»å–fusion_alphaå‚æ•°
                    fusion_alpha = calibrate_cfg.get('fusion_alpha', 0.7)

                    # GR-CLIPæ ¡å‡†
                    calibrated_weights = calibrate_with_gr_clip(
                        visual_prototypes, clip_weights, text_mean, image_mean, alpha=fusion_alpha
                    )
                    
                    if i % 1000 == 0:
                        print(f"\n[GR-CLIP] Updated at step {i}")
                        print(f"  Text mean norm: {text_mean.norm():.4f}")
                        print(f"  Image mean norm: {image_mean.norm():.4f}")
                        print(f"  Modality gap: {(text_mean - image_mean).norm():.4f}")
                        print(f"  Fusion alpha: {fusion_alpha}")
                        print(f"  Cached classes: {len(pos_cache)}")

                        # è¯Šæ–­: è®¡ç®—æ ¡å‡†å‰åçš„ç›¸ä¼¼åº¦å˜åŒ–
                        if len(visual_prototypes) > 0:
                            sample_class = list(visual_prototypes.keys())[0]
                            V_c = visual_prototypes[sample_class]
                            T_c = clip_weights[:, sample_class]
                            sim_before = (V_c @ T_c).item()

                            V_c_cal = (V_c - image_mean) / (V_c - image_mean).norm()
                            T_c_cal = (T_c - text_mean) / (T_c - text_mean).norm()
                            sim_after = (V_c_cal @ T_c_cal).item()

                            print(f"  Cross-modal similarity: {sim_before:.4f} â†’ {sim_after:.4f} (gain: {sim_after-sim_before:+.4f})")

            # æ­¥éª¤4: è‡ªé€‚åº”ä½¿ç”¨GR-CLIP (ä»…å¯¹ä½ç½®ä¿¡åº¦æ ·æœ¬)
            clip_baseline_logits = 100.0 * image_features @ clip_weights

            if text_mean is not None and image_mean is not None:
                # è®¡ç®—CLIPåŸºçº¿çš„ç½®ä¿¡åº¦
                clip_confidence = clip_baseline_logits.softmax(1).max().item()
                print(clip_confidence)
                confidence_threshold = calibrate_cfg.get('confidence_threshold', 0.9)

                if clip_confidence < confidence_threshold:
                    # ä½ç½®ä¿¡åº¦: ä½¿ç”¨GR-CLIPæ ¡å‡†
                    image_features_centered = image_features - image_mean.unsqueeze(0)
                    image_features_centered = image_features_centered / image_features_centered.norm(dim=1, keepdim=True)
                    gr_clip_logits = 100.0 * image_features_centered @ calibrated_weights
                else:
                    # é«˜ç½®ä¿¡åº¦: ä¿æŒåŸå§‹CLIP
                    gr_clip_logits = clip_baseline_logits
            else:
                # ç¼“å­˜ä¸è¶³,ä½¿ç”¨åŸå§‹CLIP
                gr_clip_logits = clip_baseline_logits

            # æ­¥éª¤5: è®¡ç®—TDAç¼“å­˜è´¡çŒ®
            # âœ… ä¿®å¤3: ä½¿ç”¨åŸå§‹ç‰¹å¾ä¿æŒTDAé€»è¾‘ä¸€è‡´æ€§ (åˆ†ç¦»GR-CLIPå’ŒTDAè·¯å¾„)
            tda_logits = torch.zeros_like(gr_clip_logits)
            if pos_enabled and pos_cache:
                tda_logits += compute_cache_logits(image_features, pos_cache,
                                                   pos_params['alpha'], pos_params['beta'], clip_weights)
            if neg_enabled and neg_cache:
                tda_logits -= compute_cache_logits(image_features, neg_cache,
                                                   neg_params['alpha'], neg_params['beta'], clip_weights,
                                                   (neg_params['mask_threshold']['lower'], neg_params['mask_threshold']['upper']))

            # æ­¥éª¤6: èåˆGR-CLIPå’ŒTDA
            final_logits = gr_clip_logits + tda_logits

            # æ­¥éª¤7: è¯„ä¼°
            acc = cls_acc(final_logits, target)
            accuracies.append(acc)
            wandb.log({"å¹³å‡æµ‹è¯•ç²¾åº¦": sum(accuracies)/len(accuracies)}, commit=True)

            if i % 1000 == 0:
                print(f"---- TDAæµ‹è¯•ç²¾åº¦: {sum(accuracies)/len(accuracies):.2f}. ----")
        
        print(f"---- æœ€ç»ˆTDAæµ‹è¯•ç²¾åº¦: {sum(accuracies)/len(accuracies):.2f}. ----\n")
        with open('outputs/result.txt', 'a') as f:
            f.write("Top1- {:.2f}\n".format(sum(accuracies)/len(accuracies)))
        return sum(accuracies)/len(accuracies)



def main():
    """
    è¿è¡ŒTDAæµ‹è¯•æ—¶é€‚åº”å®éªŒçš„ä¸»å‡½æ•°
    
    è¯¥å‡½æ•°åè°ƒå®Œæ•´çš„TDAæµæ°´çº¿:
    1. åŠ è½½å’Œé…ç½®CLIPæ¨¡å‹
    2. é¡ºåºå¤„ç†å¤šä¸ªæ•°æ®é›†  
    3. ä¸ºæ¯ä¸ªæ•°æ®é›†è¿è¡ŒTDAé€‚åº”
    4. è®°å½•ç»“æœåˆ°wandbè¿›è¡Œå®éªŒè·Ÿè¸ª
    
    å‘½ä»¤è¡Œç”¨æ³•:
    python tda_runner.py --config configs/ --datasets I/A/V/R/S --backbone RN50 --wandb-log
    """
    args = get_arguments()
    config_path = args.config

    # åˆå§‹åŒ–CLIPæ¨¡å‹(ResNet-50æˆ–ViT-B/16)
    # clip_modelåŒ…å«å›¾åƒç¼–ç å™¨E_vå’Œæ–‡æœ¬ç¼–ç å™¨E_t
    clip_model, preprocess = clip.load(args.backbone)
    clip_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼(æ— æ¢¯åº¦è®¡ç®—)

    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    random.seed(1)
    torch.manual_seed(1)

    # åˆå§‹åŒ–wandbå®éªŒè·Ÿè¸ª
    if args.wandb:
        date = datetime.now().strftime("%b%d_%H-%M-%S")
        group_name = f"{args.backbone}_{args.datasets}_{date}"
    
    # åœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šé¡ºåºè¿è¡ŒTDA
    # æ”¯æŒOODåŸºå‡†(I/A/V/R/S)å’Œè·¨åŸŸåŸºå‡†
    datasets = args.datasets.split('/')
    for dataset_name in datasets:
        print(f"æ­£åœ¨å¤„ç† {dataset_name} æ•°æ®é›†.")
        
        # ä»YAMLé…ç½®åŠ è½½æ•°æ®é›†ç‰¹å®šçš„è¶…å‚æ•°
        cfg = get_config_file(config_path, dataset_name)
        print("\nè¿è¡Œæ•°æ®é›†é…ç½®:")
        print(cfg, "\n")
        
        # å‡†å¤‡æ•°æ®é›†å’ŒCLIPç»„ä»¶
        # test_loader: batch_size=1çš„DataLoaderç”¨äºæµå¼é€‚åº”
        # classnames: æ•°æ®é›†çš„ç±»åˆ«åç§°åˆ—è¡¨
        # template: æ–‡æœ¬ç”Ÿæˆçš„æç¤ºæ¨¡æ¿
        test_loader, classnames, template = build_test_data_loader(dataset_name, args.data_root, preprocess)
        
        # ä½¿ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥W_c [d, num_classes]ï¼Œd=512
        clip_weights = clip_classifier(classnames, template, clip_model)

        # ä¸ºæ­¤æ•°æ®é›†åˆå§‹åŒ–wandbè¿è¡Œ
        if args.wandb:
            run_name = f"{dataset_name}"
            run = wandb.init(project="ETTA-CLIP", config=cfg, group=group_name, name=run_name)

        # è¿è¡ŒTDAæµ‹è¯•æ—¶é€‚åº”ç®—æ³•
        # å¤„ç†æ‰€æœ‰æµ‹è¯•æ ·æœ¬åè¿”å›æœ€ç»ˆç²¾åº¦
        acc = run_test_tda(cfg['positive'], cfg['negative'],cfg['calibrate'], test_loader, clip_model, clip_weights)

        # è®°å½•æœ€ç»ˆç»“æœå¹¶æ¸…ç†
        if args.wandb:
            wandb.log({f"{dataset_name}": acc})
            run.finish()

if __name__ == "__main__":
    main()