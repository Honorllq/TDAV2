# Efficient Test-Time Adaptation of Vision-Language Models

Adilbek Karmanov $^{1*}$  Dayan Guan $^{2*}$  Shijian Lu $^{1,2\dagger}$  Abdulmotaleb El Saddik $^{1,3}$  Eric Xing $^{1,4}$

<sup>1</sup>Mohamed bin Zayed University of Artificial Intelligence <sup>2</sup>Nanyang Technological University <sup>3</sup>University of Ottawa <sup>4</sup>Carnegie Mellon University

# Abstract

Test-time adaptation with pre-trained vision-language models has attracted increasing attention for tackling distribution shifts during the test time. Though prior studies have achieved very promising performance, they involve intensive computation which is severely unaligned with test-time adaptation. We design TDA, a training-free dynamic adapter that enables effective and efficient test-time adaptation with vision-language models. TDA works with a lightweight key-value cache that maintains a dynamic queue with few-shot pseudo labels as values and the corresponding test-sample features as keys. Leveraging the key-value cache, TDA allows adapting to test data gradually via progressive pseudo label refinement which is super-efficient without incurring any backpropagation. In addition, we introduce negative pseudo labeling that alleviates the adverse impact of pseudo label noises by assigning pseudo labels to certain negative classes when the model is uncertain about its pseudo label predictions. Extensive experiments over two benchmarks demonstrate TDA's superior effectiveness and efficiency as compared with the state-of-the-art. The code has been released in https://kdiaaa.github.io/tda/.

# 1. Introduction

Recent advances in vision-language models [19, 31, 43] have opened a new door for integrating human language into various computer vision tasks. Take CLIP [31] as an example. It can enable zero-shot image classification by leveraging a shared embedding space that is learnt from web-scale image-text pairs. Within this shared space, images can be directly recognized by matching their features with the text embeddings of CLIP classes. At the other end, CLIP often faces challenges while handling various specific downstream images, especially when the downstream images have clear domain and distribution shifts as compared

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-27/b1902ca8-9df0-4e54-a48a-be1e5991e5a5/c2c36442bcb6e9db72dbec1bed2d0c1aa0e2d34f24cbdab77e709d1918bbc2a6.jpg)  
(a) Test-time Prompt Tuning [9, 35]

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-27/b1902ca8-9df0-4e54-a48a-be1e5991e5a5/92f92bce1c305bda7e1ed38c673516aa49347d196284b5e87a2171096d6e0863.jpg)  
(b) Training-free Dynamic Adapter (Ours)  
Figure 1. Comparison of our proposed Training-free Dynamic Adapter (TDA) with Test-time Prompt Tuning TPT [35] and its enhancement DiffTPT [9]: both TPT and DiffTPT require significant computational resources to optimize the learnable prompt via backpropagation; TDA is a dynamic cache that is training-free without any backpropagation, making it efficient for test-time adaptation in various real-world scenarios.

with the CLIP training images.

Several recent studies [2, 3, 40] introduce a new paradigm called test-time adaptation for mitigating the domain shift. The idea of test-time adaptation is well aligned with real-world scenarios where a model needs to adapt to new environments quickly. Despite its great research and

application values, test-time adaptation of vision-language models has been largely neglected. Recently, Test-time Prompt Tuning, as introduced in TPT [35] and its enhancement DiffTPT [9], attempts to adapt vision-language models by learning domain-specific prompts from test data. As illustrated in Fig. 1 (a), both TPT and DiffTPT train a learnable prompt for each test sample by feeding its augmented views into the CLIP model for generating predictions and minimizing the marginal entropy of confident predictions. Despite its decent performance, the prompt optimization in both TPT [51] and DiffTPT [9] is computationally intensive which hinders its applications in various real-world scenarios.

We design a training-free dynamic adapter (TDA) that allows efficient and effective test-time adaptation of vision-language models without requiring any backpropagation in test time. As Fig. 1 (b) shows, TDA constructs a lightweight Dynamic Adapter that keeps a dynamic queue with the pseudo labels of a stream of test samples as values and the corresponding CLIP-extracted features as keys. TDA has two desirable features that make its test-time adaptation highly applicable in real-world scenarios. First, TDA is highly effective, as it improves the quality of pseudo labels via progressive incorporation of test predictions of lower entropy [11, 40]. Second, TDA is very efficient as the key-value cache is non-parametric and requires no backpropagation during testing. Beyond that, TDA cache is lightweight due to the few-shot setup and it can be computed with simple matrix multiplications [12, 21, 29, 48].

Note that the performance of TDA depends heavily on the pseudo labels of unlabelled test samples which are often noisy with prediction errors. Inspired by the idea of negative learning [22, 23, 33], we introduce negative pseudo labeling to reduce the impact of noisy estimated labels. Traditional pseudo labeling methods identify the presence of particular classes in unlabeled data, which may result in erroneous pseudo labels being assigned when comparable high probabilities are observed. In contrast, our designed negative pseudo labeling determines the absence of certain classes and can provide more accurate pseudo labels as the probabilities of these complementary classes are very low. Concretely, we construct an additional TDA cache that stores negative pseudo labels to complement the positive TDA cache. By combining positive and negative caches, TDA is more tolerant to noisy pseudo labels and can better generalize to testing data. Extensive experiments over two widely adopted test-time adaptation benchmarks show that TDA outperforms the state-of-the-art by large margins while significantly reducing the testing time from over 12 hours to 16 minutes on the ImageNet dataset.

In summary, the contributions of this work are threefold. First, we design a training-free dynamic adapter (TDA) that can achieve test-time adaptation of vision-language models

efficiently and effectively. To the best of our knowledge, this is the first work that investigates the efficiency issue of test-time adaptation of vision-language models. Second, we introduce negative pseudo labeling to alleviate the adverse impact of pseudo label noises which makes TDA more robust to pseudo label noises and generalizable to testing data. Third, we evaluate TDA extensively over two benchmarks, and experiments show that TDA achieves superior accuracy and efficiency compared with the state-of-the-art.

# 2. Related Work

Vision-language models [6, 17, 19, 31, 43] have demonstrated significant potential in learning semantic representations effectively by undergoing extensive training on image-text data. CLIP [31] stands out among these models for its ability to establish links between visual and textual representations, which enables it to achieve impressive zero-shot results on various downstream tasks. To enhance the transfer learning capability of the CLIP model in the downstream classification tasks, researchers have proposed integrating language prompt learners such as CoOp [51] and CoCoOp [50], as well as vision adapters such as CLIP-Adapter [10], Tip-Adapter [48], CaFo [49], TaskRes [45] and GraphAdapter [25]. Although these methods have shown considerable performance improvements, they typically require a large amount of training data in downstream tasks, making them less practical for real-world scenarios. On other hand, this work focuses on a new paradigm named test-time adaptation without accessing the original training data.

Test-time adaptation refers to the process of adapting models to testing data that may have distributional differences from the training data. It is particularly beneficial for real-world applications that require models to be deployed in diverse environments, such as autonomous driving in various weather conditions, medical diagnosis in different hospitals, and etc. Several recent works utilize each batch of testing samples to update partial weights [18, 37, 38, 44], normalization statistics [34], or a combination of both [40, 46]. To avoid updating models with multiple testing samples, MEMO [47] proposes enforcing the invariant predictions from different augmentations of each sample in the testing data stream. TPT [35] tackles the same challenge with vision-language models by fine-tuning a learnable prompt with each testing sample. DiffTPT [9] innovates test-time prompt tuning by leveraging pre-trained diffusion models to augment the diversity of test data samples used in TPT. Although TPT [35] and DiffTPT [9] are effective in addressing test-time adaptation of vision-language models, prompt learning is computationally expensive and time-consuming. This paper aims to mitigate the computational efficiency challenges of TPT and DiffTPT through

the introduction of a cache model.

Cache model benefits the adaptation techniques by providing efficient inference and non-parametric processing without requiring parameter updates. Unbounded Cache [12] and PSMM [27] have shown promising results in the text generation task by storing a large amount of the training dataset to capture long-term dependencies, however, this approach poses a challenge of memory efficiency during the test-time adaptation. A different technique [20] was proposed to mitigate this limitation by reducing the size of the cache memory through the search for application. Alternatively, Tip-Adapter [48] solves the cache memory problem by only storing few-shot samples per class to create a cache model for vision-language models. Inspired by this work, our dynamic adapter uses the same architecture for the test-time adaptation setting, where there is no access to the source data, and testing samples can only be accessed one by one. To address the lack of access to source data during testing, our adapter collects the most reliable test samples and their pseudo labels to form cache model.

# 3. Method

# 3.1. Preliminaries

CLIP [31] is a vision-language model that performs a proxy task of predicting the correct pairings of text and image. Consider an  $N$ -class classification problem where the CLIP's objective in the zero-shot setting is to match images with their most relevant textual descriptions using an image encoder  $E_{v}$  and a text encoder  $E_{t}$ . To obtain the textual descriptions,  $N$ -class names are concatenated with handcrafted prompts and then mapped into the  $c$ -channeled text embeddings  $\mathbf{W}_{c}$  using the text encoder  $E_{t}$ .

TPT [35] focuses on test-time adaptation of CLIP. In TPT, a prompt tuning method is proposed to learn an adaptive prompt  $\mathbf{p}_c$  using individual test samples. A set of augmentation functions  $\mathcal{A}$  is used to generate  $n$  randomly augmented views  $\tilde{x}_{\mathrm{test}} = \mathcal{A}_n(x_{\mathrm{test}})$  of a test sample  $x_{\mathrm{test}}$ . The objective of TPT is to reduce variation in the model's predictions across different augmentations  $\tilde{f}_{\mathrm{test}} = E_v(\tilde{x}_{\mathrm{test}})$  by minimizing the marginal entropy among the outputs of the augmented views. Furthermore, TPT also includes confidence selection to discard noisy augmentations that could result in ambiguous model predictions, as shown in Figure 1a. This is achieved by filtering out augmented views with high-entropy predictions as:

$$
P _ {\mathrm {T P T}} \left(\tilde {f} _ {\text {t e s t}}\right) = \frac {1}{\rho n} \sum_ {i = 1} ^ {n} \mathbb {1} \left[ \mathrm {H} \left(\tilde {f} _ {i} \mathbf {p} _ {c} ^ {T}\right) \leq \tau \right] \tilde {f} _ {i} \mathbf {p} _ {c} ^ {T}, \tag {1}
$$

where  $\mathrm{H}$  is the self-entropy function of the softmax logits predictions,  $\tilde{f}_i\mathbf{p}_c^T$  is the class probabilities vector of size

$N$  generated from the given  $i$ -th augmented view of the test image, and the parameter  $\tau$  determines that only  $\rho$ -percentile of confident samples with entropy values below this threshold can be selected out of  $n$  augmented views.

Tip-Adapter [48] provides a training-free solution that uses a key-value cache model and integrates knowledge from the pre-trained CLIP model with few-shot labeled samples. The cache model is created using a set of  $k$ -shot labeled samples  $x_{k}$  from  $N$  classes and their corresponding ground-truth labels  $y_{N}$ . It can be conceptualized as a linear two-layer model, where the first layer contains train image features  $\mathbf{F}_{\mathrm{train}} = E_v(x_k)$  and the second layer consists of one-hot vectors  $\mathbf{L}_{\mathrm{train}}$  encoded from the labels  $y_{N}$ . Given test image features  $f_{\mathrm{test}}$  generated from the CLIP's image encoder  $E_{v}$ , the prediction from the cache model can be calculated as follows:

$$
P _ {\mathrm {c a c h e}} (f _ {\mathrm {t e s t}}) = A (f _ {\mathrm {t e s t}} \mathbf {F} _ {\mathrm {t r a i n}} ^ {T}) \mathbf {L} _ {\mathrm {t r a i n}}, \qquad (2)
$$

where  $A(z) = \alpha \exp(-\beta (1 - z))$  is an adaptation function within a weighting factor  $\alpha$  and a sharpness ratio  $\beta$ . During inference, the prediction of Tip-Adapter is computed by combining the pre-trained CLIP model and the cache model as:  $P_{\mathrm{TA}}(f_{\mathrm{test}}) = P_{\mathrm{cache}}(f_{\mathrm{test}}) + f_{\mathrm{test}} \mathbf{W}_c^T$ .

# 3.2. Training-free Dynamic Adapter

During testing, pre-trained vision-language models like CLIP may encounter distribution shifts that degrade the classification performance. To address this issue, existing test-time prompt tuning methods train a learnable prompt by enforcing consistency across different image augmentations during testing. However, it requires a large number of augmentation operations on each test image and computationally-heavy optimization steps to learn the prompt, limiting its applicability in various real-world settings.

In this paper, our motivation is to design an efficient method for test-time adaptation of the pre-trained vision-language models like CLIP. Inspired by the concept of TipAdapter, we propose a training-free dynamic adapter (TDA) to enable efficient and effective test-time adaptation with CLIP. As shown in Figure 2, TDA includes two lightweight key-value caches, where each cache stores a dynamic queue of few-shot test features as keys and the corresponding pseudo labels as values. The first cache is intended for positive learning and it dynamically updates key-value pairs with high-confidence predictions to improve the accuracy. The second cache is designed for negative learning and it aims to address the adverse effects of noisy pseudo labels by introducing negative pseudo labeling to identify class absence rather than presence. By combining the positive cache and negative cache, the proposed TDA can achieve superior performance in terms of both speed and accuracy.

1. ftest 是测试样本经过数据增强后的特征  
2.n是数据增强的次数(生成的不同视图数量)  
3. pn是用于控制选择比例的参数

4.H（）是熵函数，用于计算预测的不确定性  
5.fpcT是第i个增强视图的类别概率向量  
6.1是熵阈值  
7.1[-]是示性函数，当条件满足时为1，否则为0

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-27/b1902ca8-9df0-4e54-a48a-be1e5991e5a5/50b71384853f97e842a005c676d6595076d43e5324c3ed129a60673bf4fb7a71.jpg)  
Figure 2. Overview of the proposed Training-free Dynamic Adapter (TDA). TDA constructs and updates two key-value caches to store the knowledge of a stream of test samples, and uses the two caches to generate positive and negative predictions which are combined with CLIP predictions to produce the final prediction. Specifically, the CLIP predictions are generated by performing the dot product between the image features generated by CLIP's image encoder  $E_{v}$  and the text embeddings generated by CLIP's text encoder  $E_{t}$ , using the hand-crafted prompt and class names. The two key-value caches are updated by gradually incorporating the test features and their corresponding pseudo labels calculated from CLIP's predictions, based on prediction entropy and cache capacity.

Our goal is to conduct test-time adaptation by gathering adequate knowledge from the testing data stream and improving the predictions through the adaptation of the image features. To accomplish this goal, we create positive and negative caches that capture particular characteristics of the testing data stream. In the upcoming parts, we will present the process of collecting data for each cache and define the conditions in which we can utilize cache information to adapt features and enhance model predictions.

Positive Cache. The positive cache in TDA is a key-value cache, in which keys and values are represented as a dynamic queue. It aims to collect high-quality few-shot pseudo labels  $\hat{\mathbf{L}}_{\mathfrak{p}}$  as positive values and the corresponding features  $\mathbf{Q}_{\mathfrak{p}}$  as keys. The key-value cache is initially empty and then accumulates a sufficient number of key-value pairs during the test-time adaptation. To maintain high-quality pseudo labels, TDA progressively incorporates test predictions with lower entropy while limiting shot capacity<sup>1</sup> in the positive cache. Different from a normal queue like a FIFO queue with a fixed size, the dynamic queue in our method expands in size during testing. Besides, the dynamic queue operates similarly to a priority queue, using entropy as the criterion for prioritization. Note that, each class has its own queue to maintain the order and correct data structure of each class in the cache.

Given a pre-trained CLIP model that consists of a text encoder  $E_{t}$  and an image encoder  $E_{v}$ ,  $E_{t}$  processes class

names with pre-defined prompts to generate  $c$ -channeled text embeddings  $\mathbf{W}_c$  and  $E_v$  processes each test image  $x_{\mathrm{test}}$  to produce image features  $f_{\mathrm{test}}$ . To build the positive cache, TDA first generates a pseudo label  $\hat{l}$ , a one-hot encoded vector of a categorical distribution, for each test sample  $x_{\mathrm{test}}$  by applying the softmax function to the prediction  $f_{\mathrm{test}} \mathbf{W}_c^T$ . We establish two conditions to ascertain whether and how to include the pseudo label  $\hat{l}$  and its corresponding image features  $f_{\mathrm{test}}$  into the positive cache. The first condition is defined as: if the shot number (the number of collected pairs per class) of  $\hat{\mathbf{L}}_{\mathbf{p}}$  is less than the maximum shot capacity (the maximum capacity number of pairs per class)  $k$ , TDA will add  $\hat{l}$  and  $f_{\mathrm{test}}$  as a new value and a new key to  $\hat{\mathbf{L}}_{\mathbf{p}}$  and  $\mathbf{Q}_{\mathbf{p}}$  respectively. Meanwhile, the second condition is defined as: if the shot number of  $\hat{\mathbf{L}}_{\mathbf{p}}$  has reached the maximum shot capacity  $k$ , TDA will replace an 'uncertain' key-value pair  $\{\mathbf{q}^{ent}, \hat{l}_p^{ent}\}$  with  $\{f_{\mathrm{test}}, \hat{l}\}$  when  $\mathrm{H}(f_{\mathrm{test}} \mathbf{W}_c^T) < \mathrm{H}(\mathbf{q}^{ent} \mathbf{W}_c^T)$ . Here,  $\mathrm{H}$  denotes the entropy function and the term 'uncertain' indicates that the entropy of a particular key-value pair is the highest compared to the entropy of all other key-value pairs of the same class in the cache model. By applying these two conditions, TDA can gradually integrate test predictions with lower entropy while controlling the shot capacity, which helps to ensure the collection of high-quality pseudo labels in positive cache.

During test-time adaptation, the positive cache can quickly retrieve relevant information by treating the image features  $f_{\mathrm{test}}$  generated from a test example as a query and searching the stored key-value pairs  $\{\mathbf{Q}_{\mathbf{p}}, \hat{\mathbf{L}}_{\mathbf{p}}\}$  for match-

```python
Positive_Cache = {
    'keys': Q_p, #高质量的few-shot特征
    'values': L_p, #对应的伪标签
    'max_capacity': K #缓存容量限制
}
```

ing information. The adapted predictions using the positive cache can then be obtained as:

$$
P _ {\mathrm {p o s}} \left(f _ {\mathrm {t e s t}}\right) = A \left(f _ {\mathrm {t e s t}} \mathbf {Q} _ {\mathbf {p}} ^ {\mathbf {T}}\right) \hat {\mathbf {L}} _ {\mathbf {p}}, \tag {3}
$$

where  $A$  is the adaptation function defined in Tip-Adapter.

Negative Cache. Similar to the positive cache in our TDA, the negative cache is also a dynamic queue structure with negative keys and negative values denoted as  $\mathbf{Q}_{\mathbf{n}}$  and  $\hat{\mathbf{L}}_{\mathbf{n}}$ , respectively. It aims to gather CLIP-generated image features to  $\mathbf{Q}_{\mathbf{n}}$  and the corresponding negative pseudo labels to  $\hat{\mathbf{L}}_{\mathbf{n}}$ . Unlike the pseudo labels in the positive cache, the negative pseudo labels are obtained by applying negative mask on the class probabilities as:

$$
\hat {\mathbf {L}} _ {\mathbf {n}} = - \mathbb {1} [ p _ {l} <   P (\mathbf {Q} _ {\mathbf {n}}) ], \tag {4}
$$

where higher probabilities than  $p_l$  are selected as negative pseudo labels from uncertain predictions and the uncertainty is measured by the entropy of predictions. Here,  $p_l$  represents a threshold in negative pseudo labeling, and  $\hat{\mathbf{L}}_{\mathbf{n}}$  denotes a negative pseudo label, which is a vector whose elements larger than  $p_l$  have a value -1 and otherwise 0. Different from existing negative learning methods [22, 23, 33] that select negative labels from all noisy labels, TDA selects negative pseudo labels from uncertain predictions to avoid bias to the data with certain predictions.

When constructing the negative cache, the testing feature  $f_{\mathrm{test}}$  will be included in negative cache if it satisfies the condition  $\gamma (f_{\mathrm{test}})$ : the entropy of the prediction is in the specified interval between  $\tau_{l}$  and  $\tau_{h}$ :

$$
\gamma \left(f _ {\text {t e s t}}\right): \tau_ {l} <   \mathrm {H} \left(f _ {\text {t e s t}} \mathbf {W} _ {c} ^ {T}\right) <   \tau_ {h}. \tag {5}
$$

This condition is designed to mitigate the risk of prediction errors due to high entropy or be biased to certain predictions (characterized by very low entropy), by incorporating test samples that exhibit a moderate degree of prediction uncertainty. Once the  $\gamma(f_{\mathrm{test}})$  check is completed, the remaining steps for collecting uncertain samples in the negative cache follow the same two conditions designed in the positive cache. Similar to the positive cache, TDA also limits the shot capacity  $\tilde{k}$  in the negative cache.

During test-time adaption, the testing features  $f_{\mathrm{test}}$  can be quickly adapted to target domains by retrieving the knowledge from  $\{\mathbf{Q}_{\mathbf{n}}, \hat{\mathbf{L}}_{\mathbf{n}}\}$  in the negative cache and the adapted prediction can be obtained as:

$$
P _ {\text {n e g}} \left(f _ {\text {t e s t}}\right) = - A \left(f _ {\text {t e s t}} \mathbf {Q} _ {\mathbf {n}} ^ {\mathbf {T}}\right) \hat {\mathbf {L}} _ {\mathbf {n}}, \tag {6}
$$

where  $A$  is the adaptation function defined in Tip-Adapter. The predictions of TDA can be formulated by combining the negative cache, the positive cache and the pre-trained CLIP model together as follows:

$$
P _ {\mathrm {T D A}} \left(f _ {\text {t e s t}}\right) = f _ {\text {t e s t}} \mathbf {W} _ {c} ^ {T} + P _ {\text {p o s}} \left(f _ {\text {t e s t}}\right) + P _ {\text {n e g}} \left(f _ {\text {t e s t}}\right). \tag {7}
$$

# 3.3. Relationship with TPT and Tip-Adapter

Both TPT and TDA are designed to handle the challenge of adapting models to test data that have distributional discrepancies from the training data. TPT trains a learnable prompt  $\mathbf{p}_c$  in Eq. (1) for each test sample with a large number of augmentations and such training process requires backpropagation and is computationally intensive. In contrast, our TDA is training-free as both positive cache  $\{\mathbf{Q_p},\hat{\mathbf{L}}_{\mathbf{p}}\}$  and negative cache  $\{\mathbf{Q_n},\hat{\mathbf{L}}_{\mathbf{n}}\}$  are non-parametric, which is super-efficient without incurring any backpropagation.

Our TDA employs a cache model that shares similarities with the Tip-Adapter, where features and labels are stored as key-value pairs in a memory cache. One notable distinction between the Tip-Adapter and TDA lies in the type of cache used. Specifically, the Tip-Adapter relies on a static cache  $\{\mathbf{F}_{\mathrm{train}},\mathbf{L}_{\mathrm{train}}\}$  because it is designed for a supervised adaptation setting, where the ground-truth labels  $\mathbf{L}_{\mathrm{train}}$  are predetermined and readily available. In contrast, our TDA introduces a new dynamic cache  $\{\mathbf{Q_p},\hat{\mathbf{L}}_p\}$  designed for a test-time adaptation setting, where the pseudo labels  $\hat{\mathbf{L}}_{\mathbf{p}}$  are generated on-the-fly from a stream of test samples. Furthermore, TDA incorporates a novel negative cache model  $\{\mathbf{Q_n},\hat{\mathbf{L}}_n\}$  that enhances testing predictions by utilizing indirect knowledge that a test image does not belong to certain negative classes. By combining positive and negative caches, our proposed TDA is more robust to noisy pseudo labels and can generalize well to testing data.

# 4. Experiments

# 4.1. Experimental Setup

Benchmarks. We conducted main experiments on two benchmarks: out-of-distribution (OOD) benchmark and cross-domain benchmark, both applied in the previous work [35] that adapts vision-language models in test time. The OOD benchmark serves as a measure of the robustness of our approach by involving assessment on 4 out-of-distribution datasets derived from ImageNet [5]: ImageNet-A [16], ImageNet-V2 [32], ImageNet-R [15], and ImageNet-S [41]. This benchmark is specifically designed to evaluate a model's capacity to generalize to new and unseen data. The cross-domain benchmark, on the other hand, is involved to evaluate the model's performance across 10 diverse image classification datasets, each from a distinct domain with different classes: Aircraft [26], Caltech101 [8], Cars [24], DTD [4], EuroSAT [14], Flower102 [28], Food101 [1], Pets [30], SUN397 [42], and UCF101 [36]. This benchmark provides a comprehensive evaluation of the model's adaptability during test time across various class spaces.

Table 1. Results on the OOD Benchmark. Our TDA is compared with several state-of-the-art methods designed for vision-language models: the baseline method CLIP, three train-time adaptation methods (i.e., CoOp, CoCoOp, and Tip-Adapter), and two test-time adaptation methods (i.e., TPT and DiffTPT). All the compared methods are built upon CLIP-ResNet-50 or CLIP-ViT-B/16 baselines. The two evaluation metrics Average and OOD Average are calculated by taking the mean accuracy across all five datasets and four OOD datasets excluding ImageNet. The results of CLIP, CoOp, CoCoOp, and TPT are obtained from the TPT paper, the results of DiffTPT are obtained from the DiffTPT paper, while the results of Tip-Adapter are reproduced using the official codes.  

<table><tr><td>Method</td><td>ImageNet</td><td>ImageNet-A</td><td>ImageNet-V2</td><td>ImageNet-R</td><td>ImageNet-S</td><td>Average</td><td>OOD Average</td></tr><tr><td>CLIP-ResNet-50</td><td>59.81</td><td>23.24</td><td>52.91</td><td>60.72</td><td>35.48</td><td>46.43</td><td>43.09</td></tr><tr><td>CoOp</td><td>63.33</td><td>23.06</td><td>55.40</td><td>56.60</td><td>34.67</td><td>46.61</td><td>42.43</td></tr><tr><td>CoCoOp</td><td>62.81</td><td>23.32</td><td>55.72</td><td>57.74</td><td>34.48</td><td>46.81</td><td>42.82</td></tr><tr><td>Tip-Adapter</td><td>62.03</td><td>23.13</td><td>53.97</td><td>60.35</td><td>35.74</td><td>47.04</td><td>43.30</td></tr><tr><td>TPT</td><td>60.74</td><td>26.67</td><td>54.70</td><td>59.11</td><td>35.09</td><td>47.26</td><td>43.89</td></tr><tr><td>DiffTPT</td><td>60.80</td><td>31.06</td><td>55.80</td><td>58.80</td><td>37.10</td><td>48.71</td><td>45.69</td></tr><tr><td>TDA (Ours)</td><td>61.35</td><td>30.29</td><td>55.54</td><td>62.58</td><td>38.12</td><td>49.58</td><td>46.63</td></tr><tr><td>CLIP-ViT-B/16</td><td>68.34</td><td>49.89</td><td>61.88</td><td>77.65</td><td>48.24</td><td>61.20</td><td>59.42</td></tr><tr><td>CoOp</td><td>71.51</td><td>49.71</td><td>64.20</td><td>75.21</td><td>47.99</td><td>61.72</td><td>59.28</td></tr><tr><td>CoCoOp</td><td>71.02</td><td>50.63</td><td>64.07</td><td>76.18</td><td>48.75</td><td>62.13</td><td>59.91</td></tr><tr><td>Tip-Adapter</td><td>70.75</td><td>51.04</td><td>63.41</td><td>77.76</td><td>48.88</td><td>62.37</td><td>60.27</td></tr><tr><td>TPT</td><td>68.98</td><td>54.77</td><td>63.45</td><td>77.06</td><td>47.94</td><td>62.44</td><td>60.81</td></tr><tr><td>DiffTPT</td><td>70.30</td><td>55.68</td><td>65.10</td><td>75.00</td><td>46.80</td><td>62.28</td><td>60.52</td></tr><tr><td>TDA (Ours)</td><td>69.51</td><td>60.11</td><td>64.67</td><td>80.24</td><td>50.54</td><td>65.01</td><td>63.89</td></tr></table>

Table 2. Comparisons of our TDA with CLIP-ResNet-50, TPT, and DiffTPT in terms of efficiency (Testing Time) and effectiveness (Accuracy). The final column shows the accuracy gain relative to the baseline CLIP. Note that the testing time of DiffTPT does not include the duration required for the image generation process with pre-trained diffusion models, which is an additional time-consuming factor during the testing phase.  

<table><tr><td>Method</td><td>Testing Time</td><td>Accuracy</td><td>Gain</td></tr><tr><td>CLIP-ResNet-50</td><td>12min</td><td>59.81</td><td>0</td></tr><tr><td>TPT</td><td>12h 50min</td><td>60.74</td><td>+0.93</td></tr><tr><td>DiffTPT</td><td>34h 45min</td><td>60.80</td><td>+0.99</td></tr><tr><td>TDA (Ours)</td><td>16min</td><td>61.35</td><td>+1.54</td></tr></table>

Implementation details. All the models in our experiments are built upon the pre-trained CLIP model [31] that consists of an image encoder and a text encoder. The image encoder can be either ResNet [13] or Vision Transformer [7], while the text encoder is Transformer [39]. Test-time adaptation is set for single-image scenarios, using a batch size of 1. We conduct a search for all our hyperparameters using a single ImageNet validation set. The threshold  $p_l$  for negative pseudo-labeling in Eq 4 is set as 0.03. The upper and lower thresholds  $[\tau_l, \tau_h]$  for testing feature selection in Eq 5 are set as [0.2, 0.5]. Once searched, these hyperparameters are fixed and evaluated across various new datasets. To avoid incurring backpropagation when using learnable prompt, we follow [31] to use hand-crafted

prompts. We use top-1 accuracy  $(\%)$ , a standard classification criterion, as our evaluation metric. All the experiments are conducted using a single NVIDIA Quadro RTX 6000 GPU.

# 4.2. Comparisons with State-of-the-art

In this section, we compare our proposed TDA with several state-of-the-art methods, including CLIP [31], three train-time adaptation methods, i.e., CoOp [51], CoCoOp [50], and Tip-Adapter [48], as well as two existing test-time adaptation methods TPT [35] and DiffTPT [9], all of which are designed for vision-language models. Specifically, CLIP is evaluated using an ensemble of 80 handcrafted prompts as in [31]. All train-time adaptation methods are trained on the ImageNet train set with 16 shots per class and tested on other datasets as in [35]. CoOp utilizes a learnable context module of size 4 that is fine-tuned for specific downstream tasks, while CoCoOp is the generalized version of CoOp with the input-conditional token for image features. Tip-Adapter employs the optimal hyperparameters obtained from the ImageNet validation set during evaluation. We would like to note that Tip-Adapter is unable to handle new classes during testing, limiting its implementation to OOD benchmark evaluation where the training classes encompass all the testing classes. Different from train-time adaptation methods, test-time adaptation methods (i.e., TPT, DiffTPT, and our TDA) do not utilize the ImageNet train set. Instead, they are fine-tuned with target datasets using a stream of unlabeled test samples. Follow

Table 3. Results on the Cross-Domain Benchmark. Our TDA is compared with several state-of-the-art methods designed for vision-language models: the baseline method CLIP, two train-time adaptation methods (i.e., CoOp and CoCoOp), and two test-time adaptation methods (i.e., TPT and DiffTPT). Note that Tip-Adapter is unable to be evaluated on the Cross-Domain Benchmark as it cannot handle new classes during testing. The evaluation metric Average is calculated by taking the mean accuracy across all ten datasets. The results of CLIP, CoOp, CoCoOp, and TPT are obtained from the TPT paper, while the results of DiffTPT are obtained from the DiffTPT paper.  

<table><tr><td>Method</td><td>Aircraft</td><td>Caltech101</td><td>Cars</td><td>DTD</td><td>EuroSAT</td><td>Flower102</td><td>Food101</td><td>Pets</td><td>SUN397</td><td>UCF101</td><td>Average</td></tr><tr><td>CLIP-ResNet-50</td><td>16.11</td><td>87.26</td><td>55.89</td><td>40.37</td><td>25.79</td><td>62.77</td><td>74.82</td><td>82.97</td><td>60.85</td><td>59.48</td><td>56.63</td></tr><tr><td>CoOp</td><td>15.12</td><td>86.53</td><td>55.32</td><td>37.29</td><td>26.20</td><td>61.55</td><td>75.59</td><td>87.00</td><td>58.15</td><td>59.05</td><td>56.18</td></tr><tr><td>CoCoOp</td><td>14.61</td><td>87.38</td><td>56.22</td><td>38.53</td><td>28.73</td><td>65.57</td><td>76.20</td><td>88.39</td><td>59.61</td><td>57.10</td><td>57.23</td></tr><tr><td>TPT</td><td>17.58</td><td>87.02</td><td>58.46</td><td>40.84</td><td>28.33</td><td>62.69</td><td>74.88</td><td>84.49</td><td>61.46</td><td>60.82</td><td>57.66</td></tr><tr><td>DiffTPT</td><td>17.60</td><td>86.89</td><td>60.71</td><td>40.72</td><td>41.04</td><td>63.53</td><td>79.21</td><td>83.40</td><td>62.72</td><td>62.67</td><td>59.85</td></tr><tr><td>TDA (Ours)</td><td>17.61</td><td>89.70</td><td>57.78</td><td>43.74</td><td>42.11</td><td>68.74</td><td>77.75</td><td>86.18</td><td>62.53</td><td>64.18</td><td>61.03</td></tr><tr><td>CLIP-ViT-B/16</td><td>23.22</td><td>93.55</td><td>66.11</td><td>45.04</td><td>50.42</td><td>66.99</td><td>82.86</td><td>86.92</td><td>65.63</td><td>65.16</td><td>64.59</td></tr><tr><td>CoOp</td><td>18.47</td><td>93.70</td><td>64.51</td><td>41.92</td><td>46.39</td><td>68.71</td><td>85.30</td><td>89.14</td><td>64.15</td><td>66.55</td><td>63.88</td></tr><tr><td>CoCoOp</td><td>22.29</td><td>93.79</td><td>64.90</td><td>45.45</td><td>39.23</td><td>70.85</td><td>83.97</td><td>90.46</td><td>66.89</td><td>68.44</td><td>64.63</td></tr><tr><td>TPT</td><td>24.78</td><td>94.16</td><td>66.87</td><td>47.75</td><td>42.44</td><td>68.98</td><td>84.67</td><td>87.79</td><td>65.50</td><td>68.04</td><td>65.10</td></tr><tr><td>DiffTPT</td><td>25.60</td><td>92.49</td><td>67.01</td><td>47.00</td><td>43.13</td><td>70.10</td><td>87.23</td><td>88.22</td><td>65.74</td><td>62.67</td><td>65.47</td></tr><tr><td>TDA (Ours)</td><td>23.91</td><td>94.24</td><td>67.28</td><td>47.40</td><td>58.00</td><td>71.42</td><td>86.14</td><td>88.63</td><td>67.62</td><td>70.66</td><td>67.53</td></tr></table>

ing TPT and DiffTPT, we compare TDA with the state-of-the-art over two public benchmarks: OOD benchmark and cross-domain benchmark.

Results on the OOD Benchmark. We first compare TDA with state-of-the-art methods over the OOD benchmark. Table 1 presents the experimental results, highlighting the superior performance of the proposed TDA compared to both TPT and DiffTPT across various OOD datasets derived from ImageNet. Specifically, TDA outperforms TPT on both ResNet-50 and ViT-B/16 architectures, improving OOD accuracy by  $2.74\%$  and  $3.08\%$  on average, respectively. Furthermore, compared to DiffTPT, TDA exhibits an average accuracy improvement of  $0.94\%$  and  $3.37\%$  in the OOD benchmark for ResNet-50 and ViT-B/16, respectively. These results validate the effectiveness of TDA in enhancing test-time adaptation performance on various OOD test datasets.

In order to provide a more comprehensive evaluation of our proposed method's efficiency and effectiveness, we compared it with the baseline CLIP-ResNet-50 and two existing test-time adaptation methods (i.e., TPT and DiffTPT). This comparison encompasses both testing time and testing accuracy, and the corresponding results are shown in Table 2. This evaluation is performed on the ImageNet validation dataset, which consists of 50,000 images, using a single NVIDIA Quadro RTX 6000 GPU. When compared to the baseline CLIP-ResNet-50, the proposed TDA demonstrates a significant improvement in testing accuracy  $(+1.54\%)$ , with only a minimal sacrifice in testing efficiency (requiring an additional  $4\mathrm{min}$ ). In comparison to TPT and DiffTPT, the proposed TDA demonstrates not only superior

testing accuracy but also significantly improved efficiency. It reduces the testing time dramatically from  $12\mathrm{h}50\mathrm{min}$  by TPT and even more from  $34\mathrm{h}45\mathrm{min}$  by DiffTPT, down to just 16 minutes. Without including the image generation time, DiffTPT consumes clearly more test time than TPT as it involves a time-consuming multi-step prompt updating process whereas TPT requires a single step only. The experimental results strongly validate the effectiveness and efficiency of our proposed method, establishing its suitability for real-world applications.

We then compare TDA with state-of-the-art methods over cross-domain benchmark. The results, presented in Table 3, demonstrate that TDA not only surpasses the performance of the TPT method but also shows a significant advantage over its improvement method DiffTPT. Specifically, when utilizing CLIP-ResNet-50 and CLIP-ViT-B/16 as the backbone, TDA achieves an improvement in average accuracy over TPT by  $3.37\%$  and  $2.43\%$ , respectively. These improvements, along with a  $1.18\%$  and  $2.06\%$  gain over DiffTPT for the respective backbones, further verify the effectiveness of TDA in adapting to diverse class datasets during test time. This attribute holds significant value for vision-language models such as CLIP, as it enables them to classify arbitrary classes in image classification without the need for additional training.

# 4.3. Ablation Studies

In this section, we perform ablation studies to examine the effectiveness of our designs. All the ablation studies are conducted over the ImageNet dataset, where TDA can achieve an accuracy of  $61.35\%$  under default settings. TDA consists of a Positive Cache and a Negative Cache, which

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-27/b1902ca8-9df0-4e54-a48a-be1e5991e5a5/6db2322886ad335e2b48f314104b1ecbb0971810d0671e01e8a85dadb3c4af1c.jpg)  
Figure 3. Ablation studies on two cache designs in TDA: Positive Cache and Negative Cache. All the models are built upon the baseline model CLIP-ResNet-50.

perform positive and negative pseudo labeling within our designed dynamic adapter, respectively. We first assess the efficacy of the two cache designs in TDA. As shown in Figure 3, both Positive Cache and Negative Cache significantly surpass the baseline model CLIP, demonstrating that test-time adaptation can be improved by introducing a dynamic adapter with either positive pseudo labeling or negative pseudo labeling. Besides, the two cache designs in TDA can complement each other as TDA (i.e., the combination of the two designs) clearly outperforms either Positive Cache or Negative Cache on the challenging ImageNet dataset. Moreover, we extend our ablation studies to the cross-domain benchmark, and results show that the positive cache achieved  $60.38\%$  accuracy, the negative cache  $60.11\%$ , while their combination yielded a  $61.03\%$  accuracy, thereby highlighting the significance of each type of cache in enhancing TDA's performance.

We proceed to perform parameter studies on the shot capacity, which refers to the maximum number of key-value pairs per class, in both Positive Cache and Negative Cache models. These studies aims to find the optimal balance between the diversity and accuracy of the key-value pairs. Figure 4 shows that the performance of both cache models is significantly affected when the shot capacity is either too low or too high. We find that the shot capacity is set as 3 for the Positive Cache and 2 for the Negative Cache yields the best performance. This is because an appropriate shot capacity ensures both high-quality pseudo labels (paired values) and diverse image features (paired keys) in the cache models. The negative cache is sensitive to shot numbers due to its role in storing probabilities for various negative pseudo labels, each representing a class to be excluded from the model. Contrary to the intuition that a larger negative

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-27/b1902ca8-9df0-4e54-a48a-be1e5991e5a5/ee00754c9de8daaee0751980d33769e2bcc9e02a486416e6833b9e6aa49ee497.jpg)  
Figure 4. Parameter studies on the Shot Capacity in Positive Cache and Negative Cache.

cache might be beneficial, a larger negative cache leads to more high-entropy, noisier pseudo labels, as highlighted in self-training studies like [11], thereby lowering the performance. Conversely, the positive cache, which stores single and high-confidence prediction, is less affected by variations in shot capacity, thereby maintaining consistent accuracy across different shot capacities. To facilitate practical applications, the shot capacity settings are fixed and directly applied to new datasets without the need for additional parameter adjustments.

# 5. Conclusion

In this work, we have presented TDA, a dynamic adapter for efficient and effective test-time adaptation of vision-language models. The proposed method employs a key-value cache, which maintains a dynamic queue with test-sample features as keys and corresponding few-shot pseudo labels as values, allowing for gradual adaptation to test data through progressive pseudo label improvement. Moreover, TDA introduces a negative cache to mitigate the undesirable effects of noisy pseudo labels by assigning negative pseudo labels to certain classes when the model is uncertain about its predictions. The results of extensive experiments over two benchmarks demonstrate that TDA outperforms state-of-the-art test-time adaptation methods while significantly reducing testing time. This work contributes to the research and application values of test-time adaptation and presents a promising solution to the efficiency issue of test-time adaptation of vision-language models.

# Acknowledgement

This study is supported under the Mohamed bin Zayed University of Artificial Intelligence.

# References

[1] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In European Conference on Computer Vision, 2014. 5, 11  
[2] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8344-8353, 2022. 1  
[3] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 295-305, 2022. 1  
[4] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. 5, 11  
[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009. 5  
[6] Karan Desai and Justin Johnson. VirTex: Learning Visual Representations from Textual Annotations. In CVPR, 2021. 2  
[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 6  
[8] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshops, 2004. 5, 11  
[9] Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. Diverse data augmentation with diffusions for effective test-time prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2704-2714, 2023. 1, 2, 6  
[10] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021. 2  
[11] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. 2, 8  
[12] Edouard Grave, Moustapha M Cisse, and Armand Joulin. Unbounded cache model for online language modeling with open vocabulary. Advances in neural information processing systems, 30, 2017. 2, 3  
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 6  
[14] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 2019. 5, 11

[15] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021. 5, 11  
[16] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, pages 15262-15271, 2021. 5, 11  
[17] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17980-17989, 2022. 2  
[18] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427-2440, 2021. 2  
[19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904-4916. PMLR, 2021. 1, 2  
[20] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billionscale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535-547, 2019. 3  
[21] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. 2  
[22] Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels. In Proceedings of the IEEE/CVF international conference on computer vision, pages 101-110, 2019. 2, 5  
[23] Youngdong Kim, Juseung Yun, Hyounguk Shon, and Junmo Kim. Joint negative and positive learning for noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9442-9451, 2021. 2, 5  
[24] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV Workshops, 2013. 5, 11  
[25] Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, and Xinchao Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. Advances in Neural Information Processing Systems, 36, 2024. 2  
[26] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013. 5, 11  
[27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. 3  
[28] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008. 5, 11

[29] Emin Orhan. A simple cache model for image recognition. Advances in Neural Information Processing Systems, 31, 2018. 2  
[30] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012. 5, 11  
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 3, 6  
[32] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imageNet? In ICML, 2019. 5, 11  
[33] Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. arXiv preprint arXiv:2101.06329, 2021. 2, 5  
[34] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539-11551, 2020. 2  
[35] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. 1, 2, 3, 5, 6  
[36] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. CoRR, abs/1212.0402, 2012. 5, 11  
[37] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In ICML, 2020. 2  
[38] Thomas Varsavsky, Maurizio Orbes-Arteaga, Carole H Sudre, Mark S Graham, Parashkev Nachev, and M Jorge Cardoso. Test-time unsupervised domain adaptation. In Medical Image Computing and Computer Assisted Intervention—MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I 23, pages 428–436. Springer, 2020. 2  
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 6  
[40] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2  
[41] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019. 5, 11  
[42] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010. 5, 11

[43] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liquun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15671-15680, 2022. 1, 2  
[44] Chenyu Yi, SIYUAN YANG, Yufei Wang, Haoliang Li, Yap peng Tan, and Alex Kot. Temporal coherent test time optimization for robust video classification. In The Eleventh International Conference on Learning Representations, 2023. 2  
[45] Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, and Xinchao Wang. Task residual for tuning vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10899-10909, 2023. 2  
[46] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: Learning to adapt to domain shift. Advances in Neural Information Processing Systems, 34:23664-23678, 2021. 2  
[47] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. 2  
[48] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun-chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In European Conference on Computer Vision, pages 493-510. Springer, 2022. 2, 3, 6  
[49] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15211-15222, 2023. 2  
[50] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816-16825, 2022. 2, 6  
[51] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337-2348, 2022. 2, 6

# 6. Benchmark Details

This section provides detailed information on the two benchmarks used in our work. OOD Benchmark is used

to evaluate the model robustness against natural distribution shifts using the traditional ImageNet and its out-of-distribution (OOD) versions containing images with varying styles and corruptions. Herein below, we provide a concise overview of each of the OOD datasets.

- ImageNet-V2 [32] consists of 10,000 images and 1,000 ImageNet classes, and was collected by applying an updated natural data collection pipeline to the original ImageNet data.  
- ImageNet-A [16] is a subset of 7,500 visually similar but naturally perturbed ImageNet images of 200 classes.  
- ImageNet-R [15] includes 30,000 images belonging to 200 categories of the ImageNet dataset, but with diverse artistic styles.  
- ImageNet-S [41] consists of 50,000 sketches of 1000 class objects from the ImageNet dataset, and represents a domain shift from natural images to sketches.

Cross-Domain Benchmark consists of 10 image classification datasets to evaluate the effectiveness of the method on different domains. This benchmark incorporates the following datasets: Caltech101 [8] for general image classification, OxfordPets (Pets) [30], StanfordCars (Cars) [24], Flowers102 [28], Food101 [1], and FGVCAircraft (Aircraft) [26] for fine-grained image classification, EuroSAT [14] for satellite image classification, UCF101 [36] for action classification, DTD [4] for texture classification, and SUN397 [42] for scene classification.

The detailed statistics of all the datasets are shown in Table 4.

# 7. Parameter Studies on Thresholds

This section provides more parameter studies on three thresholds defined in our work. Our experiments are conducted on the ImageNet validation set using the default settings.

The threshold for negative pseudo-labeling. In Eq 4 of our manuscript, the threshold  $p_l$  is used to select negative pseudo labels by applying the negative mask. We conduct parameter studies on  $p_l$  and the results are illustrated in Figure 5. The best performance is achieved when  $p_l$  is set to 0.03 and subsequent increases in  $p_l$  do not yield notable improvement or degradation in the results, indicating the stability of this parameter. It can be noticed that the performance deteriorates when  $p_l$  is less than 0.03 because the confident classes with low probabilities should not be included in negative pseudo labels.

Table 4. Datasets statistics.  

<table><tr><td>Dataset</td><td>Classes</td><td>Test Size</td></tr><tr><td>ImageNet</td><td>1,000</td><td>50,000</td></tr><tr><td>ImageNet-V2</td><td>1,000</td><td>10,000</td></tr><tr><td>ImageNet-S</td><td>1,000</td><td>50,000</td></tr><tr><td>ImageNet-A</td><td>200</td><td>7,500</td></tr><tr><td>ImageNet-R</td><td>200</td><td>30,000</td></tr><tr><td>Aircraft</td><td>100</td><td>3,333</td></tr><tr><td>Caltech101</td><td>100</td><td>2,465</td></tr><tr><td>Cars</td><td>196</td><td>8,041</td></tr><tr><td>DTD</td><td>47</td><td>1,692</td></tr><tr><td>EuroSAT</td><td>10</td><td>8,100</td></tr><tr><td>Flowers102</td><td>102</td><td>2,463</td></tr><tr><td>Food101</td><td>101</td><td>30,300</td></tr><tr><td>Pets</td><td>37</td><td>3,669</td></tr><tr><td>SUN397</td><td>397</td><td>19,850</td></tr><tr><td>UCF101</td><td>101</td><td>3,783</td></tr></table>

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-27/b1902ca8-9df0-4e54-a48a-be1e5991e5a5/d1e9acea3410338f89ac5f57ee279c89ef2269b2221f22547d6c212fa8c8a8d9.jpg)  
Figure 5. Parameter studies on the Negative Mask Threshold  $p_l$  for the negative pseudo-labeling in Negative Cache. The results are reported on ImageNet top-1 accuracy using only the Negative Cache to produce an adapted prediction. The experiments are conducted with CLIP-ResNet50.

The threshold range for testing feature selection in the negative cache. In Eq 5 of our manuscript, the thresholds  $[\tau_l,\tau_h]$  are used to check whether the testing feature will be considered to be included in Negative Cache if the entropy of the prediction is in the specified interval. Table 5 presents the results of an ablation study focusing on the impact of adjusting the threshold range for testing feature selection in the Negative Cache. This investigation delves into the testing feature selection of uncertain sam

<table><tr><td colspan="3">Minimum entropy features</td><td colspan="3">Maximum entropy features</td></tr><tr><td>τl</td><td>τh</td><td>Accuracy</td><td>τl</td><td>τh</td><td>Accuracy</td></tr><tr><td>0.0</td><td>1.0</td><td>60.69</td><td>0.2</td><td>0.4</td><td>60.51</td></tr><tr><td>0.0</td><td>0.2</td><td>60.67</td><td>0.2</td><td>0.5</td><td>60.53</td></tr><tr><td>0.0</td><td>0.3</td><td>60.69</td><td>0.2</td><td>0.6</td><td>60.51</td></tr><tr><td>0.1</td><td>0.3</td><td>60.76</td><td>0.3</td><td>0.5</td><td>60.30</td></tr><tr><td>0.1</td><td>0.4</td><td>60.77</td><td>0.3</td><td>0.7</td><td>60.30</td></tr><tr><td>0.2</td><td>0.4</td><td>60.81</td><td>0.4</td><td>0.6</td><td>60.16</td></tr><tr><td>0.2</td><td>0.5</td><td>60.83</td><td>0.4</td><td>0.7</td><td>60.16</td></tr><tr><td>0.2</td><td>0.6</td><td>60.81</td><td>0.5</td><td>0.7</td><td>60.34</td></tr><tr><td>0.3</td><td>0.5</td><td>60.34</td><td>0.5</td><td>0.8</td><td>60.34</td></tr><tr><td>0.3</td><td>0.6</td><td>60.35</td><td>0.6</td><td>0.8</td><td>60.35</td></tr></table>

Table 5. Ablation study of the impact of varying Threshold Range  $[\tau_l, \tau_h]$  for testing feature selection in the Negative Cache. The study investigates the testing feature selection of the uncertain samples in two ways: choosing the minimum and maximum entropy features in the given range. The results are reported on ImageNet top-1 accuracy using only the Negative Cache to produce an adapted prediction. The experiments are conducted with CLIP-ResNet50.

plies using two distinct approaches: one involving values closer to the minimum range threshold  $(\tau_{l})$  and the other to the maximum range threshold  $(\tau_{h})$ , achieved by reversing the second condition:  $\mathrm{H}(f_{\mathrm{test}}\mathbf{W}_c^T) < \mathrm{H}(\tilde{\mathbf{q}}^{ent}\mathbf{W}_c^T)$  or  $\mathrm{H}(f_{\mathrm{test}}\mathbf{W}_c^T) > \mathrm{H}(\tilde{\mathbf{q}}^{ent}\mathbf{W}_c^T)$ . By collecting the lowest entropy features within the range [0.2, 0.5], the highest result is attained  $60.83\%$ . Opting values below 0.2 indicates the selection of confident samples for the Negative Cache, resulting in a reduction in the confidence of CLIP's prediction. Furthermore, a shift by 0.1 from [0.2, 0.5] to [0.3, 0.6] in the thresholds leads to an inclusion of more noisy samples during the early collection phase of the negative cache, resulting in a  $0.48\%$  decrease in performance. Selecting maximum entropy features with the same threshold range displays a slight decline in performance compared to the minimum entropy feature selection within the specified range. Hence, the most valuable uncertain samples fall within the [0.2, 0.5] range, whose entropy is closer to 0.2. The reported results exclusively utilize the Negative Cache to generate an adapted prediction.

The residual and sharpness ratios. The experiments in Table 6 show that the optimal residual ratio is 2.0 for TDA (instead of 1.0 in Tip-Adapter), indicating a higher significance of adapted features compared with CLIP features in test-time adaptation. The optimal sharpness ratio for TDA is 5.0, which is close to the 5.5 in Tip-Adapter.

<table><tr><td>Residual Ratio</td><td>0.5</td><td>1.0</td><td>2.0</td><td>3.0</td><td>4.0</td><td>5.0</td></tr><tr><td>TDA</td><td>61.07</td><td>61.20</td><td>61.35</td><td>61.29</td><td>60.90</td><td>60.63</td></tr><tr><td>Sharpness Ratio</td><td>0.5</td><td>1.0</td><td>3.0</td><td>5.0</td><td>7.0</td><td>9.0</td></tr><tr><td>TDA</td><td>60.98</td><td>61.20</td><td>61.29</td><td>61.35</td><td>61.20</td><td>61.19</td></tr></table>

Table 6. Analysis on the residual and sharpness ratios of TDA.

# 8. More Experimental Analysis

Caches built for inference. The caches are built on the fly during inference, starting empty and progressively accumulating samples. At the start of the testing phase on ImageNet, where only  $1\%$  of the data was used, we observed a slight accuracy drop of  $0.06\%$ . We also noted that bypassing cache usage at the early testing phase leads to a marginal accuracy improvement of  $0.1\%$ . We didn't adopt this approach as it increases complexity by introducing an extra hyperparameter for determining when to use caches.

Class imbalance under high shot capacity. Our analysis with a 6-shot positive cache reveals minimal class imbalance (only 4 out of 1000 ImageNet classes have less than 6 samples) but identifies a significant cache accuracy drop from  $90.3\%$  to  $86.6\%$  when shot capacity increases from 3 to 6. Such accuracy drop happens because larger cache capacities tend to accumulate noise, thereby reducing the reliability of cached labels and negatively affecting the adapted predictions. Hence, the performance decline with larger caches is mainly due to noise accumulation rather than class imbalance.

# 9. Broader Impact

The broader impact of test-time adaptation of vision-language models lies in its potential to enhance real-world applicability, improve accessibility and inclusivity, address bias and fairness concerns, and advance research and development. By allowing models to adapt to new, unseen data during inference, these models can be more versatile and adaptable, benefiting various domains such as healthcare and assistive technologies. Test-time adaptation also offers opportunities to mitigate biases, personalize user experiences, and push the boundaries of what vision-language models can achieve. However, ethical considerations must be taken into account to ensure responsible development and deployment, ensuring transparency, fairness, and accountability in the adaptation process.